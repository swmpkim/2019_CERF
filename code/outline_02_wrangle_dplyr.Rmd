---
title: 'Working with data: main dplyr verbs'
author: "Kim Cressman"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
    df_print: tibble
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(tidyr)
library(lubridate)
library(here)
library(ggplot2)
```


# dplyr verbs 

+  `filter()` to subset based on rows
+  `select()` to subset based on columns  
+  `mutate()` to add or modify values in columns  

We will use data downloaded from ebird to explore these.  

First, let's read in the ebird data and explore it:  

```{r}
ebird <- read.csv(here::here("data", "eBird_workshop.csv"), stringsAsFactors = FALSE)
   # 183,742 rows
```



Locate duplicates:    

```{r}
   # 22 duplicates = 11 that need to go
```

Use `dplyr::distinct()` to keep only unique rows.  

```{r}

glimpse(ebird)   # 183,731 rows
```




## Choose _rows_: `filter()`  

The data argument comes first, then the condition.    

```{r}

```

Or pipe!  

```{r}

```

Assign to a data frame and explore:  

```{r}

```

Notice a couple things here:  

+  We use `==` to specify an exact condition. If we were working with numbers, we could use:  
    +  `<`  
    +  `<=`  
    +  `==`  
    +  `!=` (**not** equal to)  
    +  `>=`  
    +  `>`  
+  The condition specified inside `filter()` MUST return either true or false, for each row.  


### multiple conditions  

We can filter based on more than one criterion, e.g. if we want all birds from Alaska, but only in the year 2008:  

```{r}

```


**Note**: It doesn't matter if you select `state` or `year` first - you can do either.  


```{r}

```
 

***

### Your Turn 1  

How could we pull out birds in Alaska (AK), before 2010? (Hint: you can use the same symbols on year that you would use with any other numbers)  

```{r}

```

***  

### multiple conditions, cont.  

Make a vector of the states you're interested in (AK, AL, MS):  

```{r}

```

And then use `%in%` in the filter statement, to say, e.g. "any of the states _in_ this vector": 

```{r}


# make sure the states we named, and only the states we named, are represented:


```

You could also skip the step of naming the vector separately:  

```{r}

```


### Your Turn 2  

How would you filter the data to contain only the species "American Coot" from MS and FL (your instructors' states), in all years *except* 2010? Assign this object to a data frame and verify (using `unique()`) that you did it right.    

```{r}

```


***
***


## Choose _columns_: `select()`  

First way to use this function is to select the columns we **do** want to keep (species, state, year):  

```{r}

```


Alternately, could use a `-` in front of the columns we **don't** want to keep (samplesize, presence):  

```{r}

```


Order **does** matter in `select()`; remember it didn't matter in `filter()`. In `select()`, selections are made in the order specified - so if you want to rearrange the columns of your data, you can do it with this command! Select year, state, and species; this moves year to the front.  

```{r}

```


You can also move just one or a few columns to the beginning by specifying it/them, and then `everything()` - it keeps all other columns, in their original order. Try it with year, everything.  

```{r}

```


### Aside: helper functions  

Sometimes you have a lot of columns that start with the same prefix, or that end with the same appended matter; maybe you want to keep all of them (or get rid of all of them). In these situations, you can use the helper functions `starts_with()` and `ends_with()`. See the `dplyr` cheatsheet for other possible helper functions.  

In our little example, let's keep columns whose names start with 's':  

```{r}
ebird %>% 
    select(starts_with("s")) %>% 
    glimpse()
```

Or columns that end with 'e':  

```{r}
ebird %>% 
    select(ends_with("e")) %>% 
    glimpse()
```

Check out the data wrangling cheat sheet for more helper functions. 

### end aside  



### Your Turn 3  

From the ebird data, subset to only include the species American Coot, from the states FL, AL, and MS. Keep only the state, year, and presence columns. What is the proper order of operations in this case?  

```{r}

```


***  
***  

## Setup for the next bit of the workshop  

We're going to switch datasets here, to one that's a little more complicated: water quality data! Shannon has downloaded monitoring data from several National Estuarine Research Reserves for the year 2016, and has already winnowed it down a bit to daily averages of several parameters that we measure. We will use your newfound `select()` and `filter()` skills to make it an even smaller data frame, and then we'll learn how to calculate new columns and do group-wise summaries.  

```{r}
wq <- read.csv(here::here("data", "daily_wq.csv"), stringsAsFactors = FALSE)
```


### Your Turn 4  

Because this is so important, I will provide working code in a chunk below. But please attempt it yourself first!!!  

We have read in the daily water quality data for a few stations. Create a new data frame called `wq_trimmed` where, from `wq`, you:    

+  **Select** the following columns:  station_code, month, day, temp, sal, do_pct, and depth.  
+  **Filter** for rows where `depth` is *not* missing. (Hint: `is.na` is the function that checks to see if a value *is* missing. How would you look for "not" `is.na`?   It's similar to "not equal to" from above.)

```{r}

```












### Your Turn 4 answer   

Last chance to try it yourself.... 
but if you didn't get it to work, run this chunk:  

```{r}
wq_trimmed <- wq %>% 
    select(station_code, month, day, temp, sal, do_pct, depth) %>% 
    filter(!is.na(depth))
```




How much has been removed from the data frame?  

```{r}
# look at dimensions of each


# can subtract both rows and columns at once!


# why did all those rows go away?
```


Some SWMP stations report `depth`, which is water depth above the data logger; and others report `level`, which is water surface relative to a standard datum (NAVD88). There is an important distinction that's irrelevant for what we want to cover today, so here we have only kept the stations that report `depth`.  



## Modifying data frames with `mutate()`  


`mutate()` operates on rows. Let's add a column called `depth_ft` to the data frame; it is depth in meters * 3.28.      

```{r}

```


You can also use other columns. You can use them in mathematical expressions, or just combine them:        

```{r}

```


You can even use a column immediately after creating it!  

```{r}

```




### Your Turn 5  

There are two parts to this. You can approach them separately or within the same series of pipes. Remember to save the result as the new, better, `wq_trimmed` data frame!   

1.  Now that we've started creating more columns, it might make sense to get rid of some old ones. Remove `monthday` and `meaningless_thing` from the `wq_trimmed` data frame.  
2.  The same person that wants to see `depth` in feet rather than meters *also* wants you to turn `temp` into Fahrenheit, from Celsius. You've looked up the conversion. Now create a new column, `temp_f`, with the new variable.   **F = (9/5)(temp in C) + 32**


As with Your Turn 4, the answer is below. No peeking!  


```{r}

```










### Your Turn 5 answer  

```{r}
wq_trimmed <- wq_trimmed %>% 
    select(-monthday, -meaningless_thing) %>% 
    mutate(temp_f = (9/5) * temp + 32)
View(wq_trimmed)
```



## Dates and Times  

Because we've just worked a little bit with mutate, and one of the things you may find yourself doing is trying to get dates and times in the right format, this seems like a good time to tackle it. We will make up a vector of date-like things and learn how to use the `lubridate` package to deal with them.  

```{r}
dates_df <- data.frame(dates01 = c("1-1-2019", "1-2-2019", "1-3-2019"),
                       dates02 = c("1/1/19", "1/2/19", "1/3/19"),
                       dates03 = c("2019-01-01", "2019-01-02", "2019-01-03"),
                       datetime = c("2019-01-01 12:00", "2019-01-01 13:00", 
                                    "2019-01-01 14:00"))

```

How are these showing up in the data frame?  

```{r}

```

Try using `as.Date` on those dates:  

```{r}

```

Enter the lubridate package. It lets you turn character strings and even numbers into dates - or date/times! - by specifying the order of what's in your data as the function name.  

That's hard to explain, so let's just do it. The first column is currently in "m-d-y" (with 4 digits in the year) format. The second is "m/d/y" - similar format, but only two digits in the year. The same lubridate function will work for both of these: `mdy()`. 

```{r}

```

Notice the returned date is "yyyy-mm-dd", which is actually a standard date format known as ISO 8601.  

For our 3rd column, it's already in that order we need, but we need it to be a date. Any guesses as to what that function is?  

```{r, eval = FALSE}
dates_df %>% 
    mutate(dates01_changed = mdy(dates01),
           dates02_changed = mdy(dates02),
           dates03_changed = ---(dates03)) %>% 
    glimpse()
```


Times make things trickier. R wants date-time columns to be in a format called `POSIX`. There is a function in base R called `as.POSIXct()` that lets you transform different values into that format. It is a burden though to specify the format you're feeding into the function. (If you don't want to take my word for it, look up the help file:  `?as.POSIXct`)

The lubridate package makes it just as simple as the dates we did above. Again, the exact function you want to use depends on the order your date appears, but all you have to do is add `_hm` or `_hms` to the end of your `mdy`/`ymd` function, depending on whether your time has seconds or not.  

```{r}
dates_df %>% 
    mutate(dates01_changed = -------,
           dates02_changed = -------,
           dates03_changed = -------,
           datetime_changed = -------) %>% 
    glimpse()
```


If you only work with times in their own column, the `hms` library may help you out. There are differences between operating systems and software versions in how Excel stores dates and times in its brain, so **be VERY CAREFUL** and check your transformed data to make sure you have the output you think you have!  


### Your Turn 6  

`mutate` can be used to add a complete vector of the same value. That is what we'll do below. Fill in the skeleton code to create a column for a full yyyy-mm-dd style date, then make a *line* graph of temperature throughout the year at the water quality stations, and color them by station code.  

Which of these stations do you think is at the NERR in Alaska?  

```{r, eval = FALSE}
wq_trimmed %>% 
    mutate(year = 2016,
           date_pasted = paste(year, month, day, sep = "-"),
           date = ----(date_pasted)) %>%   # hint: use a lubridate function!
    ggplot() +
    geom_----(mapping = ----(x = date, y = temp_f, col = ----))
```



***  
***  
***  

# Group-wise operations with `group_by()` and `summarize()`  

Say we want to find out how many birds were seen by state in the ebird dataset. We can do that. These operations are also great for lumping data into daily, monthly, or yearly averages, which we'll do on the SWMP dataset.  

First, with ebird, calculate mean and max presence by state.

```{r}

```



We can also group by combinations of multiple variables. Find mean and max presence by state and species.  

```{r}

```



### Your Turn 7  

How would you group the `wq_trimmed` dataset to calculate monthly average temperature and salinity, and their standard deviations, at each station?  

```{r}

```











### Your Turn 7 answer  

```{r}
wq_trimmed %>% 
    group_by(station_code, month) %>% 
    summarize(mean_temp = mean(temp, na.rm = TRUE),
              sd_temp = sd(temp, na.rm = TRUE),
              mean_sal = mean(sal, na.rm = TRUE),
              sd_sal = sd(sal, na.rm = TRUE)) %>% 
    View()
```


### Aside - more summarize options  

There are a couple of useful variants of `summarize` for special cases. Note that you can only perform one summary function with these (so you couldn't calculate mean *and* standard deviation in this way).    

#### `summarize_all()`  

Applies a function to all of the non-grouping variables! (Even "day", which isn't meaningful, but that's okay)  

```{r}
wq_trimmed %>% 
    group_by(station_code, month) %>% 
    summarize_all(mean, na.rm = TRUE) %>% 
    View()
```


#### `summarize_at()`  

To choose which variables to summarize  

```{r}
wq_trimmed %>% 
    group_by(station_code, month) %>% 
    summarize_at(c("temp", "sal"), mean, na.rm = TRUE) %>% 
    View()
```

### end aside  


# Sort  


You can also sort your data frame (or its summary) using `arrange()`. Let's put our ebird summary (mean and max presence, grouped by state and species) in order by species, then state:  

```{r}

```


Or put it in order by `max_presence`:  

```{r}

```


If you want the highest number at the top instead, use `desc()`:  

```{r}

```




### Your Turn 8  

Use the `wq_trimmed` data frame. Calculate monthly average temp, sal, and do_pct (at least - more variables if you like) *for each station*. Make a scatterplot using any two of these variables. Use what you've learned about `ggplot2`'s options to adjust the look and feel of the graph. Is the relationship what you expected it to be? Does it vary by site?     

```{r}

```













Here's one possible way to go about it. You could also make a new data frame of the summary variables, and use that in a separate call to `ggplot`.  

```{r}
wq_trimmed %>% 
    group_by(station_code, month) %>% 
    summarize_all(mean, na.rm = TRUE) %>% 
    ggplot() +
    geom_point(aes(x = sal, y = do_pct, col = station_code), 
               size = 3, alpha = 0.3) +
    theme_minimal() +
    facet_wrap(~station_code)
```

